# EECE 7398 FPGAs in the Cloud Project: Channel Estimation

Rana Bogrekci - Chunan Chen

## Introduction
Channel Estimation is a concept of wireless communication in which the process tries to figure out the channel characteristics. In all communication systems, data is transmitted from one place to another. The medium which a signal is transmitted through is called the channel. The channel can be both wired and wireless, and it can distort the signal while it passes. To be able to remove the distortion on the signal, we need to know what characteristics the channel carries, and in what way it distorts the signal.  Neural networks have become a popular scheme in channel estimation.
![The Process of Channel Estimation with Neural Networks](https://github.com/reyna07/chann_estimation/blob/main/img/ch_estimation_process.png)

For this project, we aim to imagine the channel as a 2D matrix having two axes depicting the time and frequency response, and turn the problem of channel estimation into an image processing problem, where 2D CNNs commonly used. To speed up the process of inference, we plan to use VCK5000. 

## Model and data acquisition
The Matlab example:[Deep Learning Data Synthesis for 5G Channel Estimation](https://www.mathworks.com/help/5g/ug/deep-learning-data-synthesis-for-5g-channel-estimation.html) provides us with a pre-trained channel evaluation model, input data for testing, and golden output data as a reference (perfect evaluation, actual channel realization).

## Project realization
We ported the channel evaluation model generated by the Matlab example to the Pytorch environment to realize the inference process. In the testbench, we use the test signals generated by the Matlab example as inputs of inference, and the output of inference are compared with the golden outputs (perfect evaluation, realization of the actual channel provided by the Matlab example). The testbench will return the MSE(Mean Square Error) of the inference output against the golden output.

Then, we will use the Vitis-AI Pytorch workflow to do the quantization and optimization to the model, finally implement the model to VCK5000.

![The Project Steps Block Diagram](https://github.com/reyna07/chann_estimation/blob/main/img/chann_block.png)

## Setting up the project
1. Follow the [Getting started with VCK5000 Versal devices in OCT](https://github.com/OCT-FPGA/versal-tutorials/blob/main/vck5000-getting-started.md) to set the environment in Open Cloud Testbed. Important information: The workflow must be selected as #Vitis-AI# instead of #Vitis#.

   
2. Go to the Vitis-AI directory

```
cd /docker/Vitis-AI
```

3. Start the Docker container

```
./docker_run.sh xilinx/vitis-ai-pytorch-cpu:latest
```

4. Setup the Environment Variable after entering the Docker container.

```
source /workspace/board_setup/vck5000/setup.sh DPUCVDX8H_6pe_dwc
```

5. Clone this github to Vitis-AI directory

```
git clone https://github.com/reyna07/chann_estimation.git
```

## Software Simulation
1. Get into the `chann_estimation` directory
```
cd chann_estimation
```

2. Run the `run.sh` script to see the software simulation result:
```
chmod +x run.sh
./run.sh
```

Then, you can see the result which shows the MES between SW simulation result and golden output:

```
Prediction of the pretrained model has started...

Prediction finished!

Prediction file is ready!

Comparing against output data:

*****************************************************************************

PASS: The output matches the golden output!

Mean Squared Error between predicted and golden output: 0.0697585865855217

*****************************************************************************

```
## Hardware Simulation 

In this part, we cover the development and deployment of a quantized neural network model for a DPU (Deep Processing Unit) using the Vitis AI tools. The process involves several key steps, including model quantization, evaluation, compilation, and DPU inference. After setting the environment variables and downloading the necessary files by following the `Setting up the project` section, this section can be directly executed. 

1. Get into the `/worckspace/chann_estimation/code` directory
```
cd /workspace/chann_estimation/code/
```

2. Run the quantization file in calibration mode to generate configuration file. In calibration mode, `batch_size` is 32.
```
python quant_rev4.py --quant_mode 'calib' --batch_size 32
```

3. Run the quantization file in test with deploy mode to generate xmodel. In test mode, the `batch_size` is 1 by default.
```
python quant_rev4.py --quant_mode 'test' --deploy
```
While generating and exporting the xmodel, the test mode also evaluates the model with dummy inputs and gives accuracy results. In command window, results shold be seen like this: 

``` 
Golden output dimension:torch.Size([8568])
Quantizer output dimension:torch.Size([8568])
Accuracy as Mean Squared Error (MSE): 0.13897
Loss: 0.36117637157440186


Model export completed!
Exporting test input data...

Test input data saved to test_input.txt
output dimension is:torch.Size([1, 1, 14, 612])
Test output data saved to test_output.txt

```

4. Compile the xmodel for DPU. 
```
vai_c_xir -x quantize_result/GraphModule_int.xmodel -a /opt/vitis_ai/compiler/arch/DPUCVDX8H/VCK50006PEDWC/arch.json -o chesti_pt -n chesti_pt
```
In above command, `quantize_result/GraphModule_int.xmodel` is the xmodel generated with quantization file. The architecture json file corresponding to the target setting is `/opt/vitis_ai/compiler/arch/DPUCVDX8H/VCK50006PEDWC/arch.json `. `chesti_pt` is the output directory in which the compiled xmodel will be placed with the name `chesti_pt.xmodel`. This compilation is done to  take the quantized INT8.xmodel and generate the deployable DPU.xmodel by running the command above.

5. Run the following command to generate the DPU inference result and compare the DPU inference result with the reference data automatically.
```
env XLNX_ENABLE_DUMP=1 XLNX_ENABLE_DEBUG_MODE=1 XLNX_GOLDEN_DIR=./quantize_result/deploy_check_data_int/GraphModule    xdputil run ./chesti_pt/chesti_pt.xmodel ./quantize_result/deploy_check_data_int/GraphModule/GraphModule__input_0.bin 2>result.log 1>&2
```
Here, `XLNX_GOLDEN_DIR=` sets the golden data, `./quantize_result/deploy_check_data_int/GraphModule` is the checking data directory(this directory contains many .bin files),
`xdputil run` runs the compiled xmodel in the path `./chesti_pt/chesti_pt.xmodel`.  `./quantize_result/deploy_check_data_int/GraphModule/GraphModule__input_0.bin` is the input bin data path. The results will be generated and saved in `result.log`.

6. Visualize the content of the result file `result.log`.
```
vim result.log
```
After executing this command, one should be able to see below lines: 
```
fillin GraphModule__input_0_fix
dump output to  0.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin
dump output to  1.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin
dump output to  2.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin
dump output to  3.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin
dump output to  4.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin
dump output to  5.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin

```
This indicates that the binary file `GraphModule__input_0_fix` was loaded as the input data for the model, and the final output from model operations was written to a binary file named `X.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin`. Here, X index represents to sequence of the layer outputs. The rest of the binary file's name indicates the fifth convolutional layer's output is being dumped. 
Quit the result.log file

## Runtime Compiling and Deployment

In this part, the created xmodel is deployed to the hardware and the inference is done in the AI Engines of VCK5000 board. After setting the environment variables and downloading the necessary files by following the `Setting up the project` section, this section can be directly executed. 

1. Get into the `/worckspace/chann_estimation/code` directory.
   
```
cd /workspace/chann_estimation/code/
```

2.  Run the `build.sh script`, which call the `ch_estimation_runner_2.cpp` and `ch_estimation_runner_3.cpp`. The .cpp files are C++ code which used Vitis-AI C++ APIs, can compile the loading xmodel and input data into VCK5000, and saving the output data process. The build.sh sets the compile environment, assign the compiled bit file name, and call the g++ compiler to compile these C++ codes.  `ch_estimation_runner_2.cpp` load the quantized xmodel and 2 original input data sets which are same as the software simulation data sets, then saved 2 output data sets to 2 text files. `ch_estimation_runner_3.cpp` load the test input data set which is generated in Quantization step.
```
chmod +x build.sh
./build.sh
```
Check the directory, you can see 2 runtime bit files: `ch_estimation_runner_2` and `ch_estimation_runner_3`.

3. Run `ch_estimation_runner_2` runtime bit file, then check the directory, you can see 2 output text files:`ouput1.txt` and `output2.txt`.
```
./ch_estimation_runner_2
```

4. Run `ch_estimation_runner_3` runtime bit file, then check the directory, you can see an output text files:`hw_test_ouput.txt`.
```
./ch_estimation_runner_3
```

5. Run the output data process Python code,
```
python hardware_result.py
```
Then you can see the MSE for 3 different hardware output:
```
MSE Loss of the hardware result for input1 is: 0.3863668739795685
MSE of the hardware result for input2 is: 0.393906831741333
MSE of the hardware result for test_input is: 0.38810738921165466
```


