# Channel Estimation with CNNs - Implementation on FPGAs in the Cloud

Rana Bogrekci - Chunan Chen

## Introduction
Channel Estimation is a concept of wireless communication in which the process tries to figure out the channel characteristics. In all communication systems, data is transmitted from one place to another. The medium which a signal is transmitted through is called the channel. The channel can be both wired and wireless, and it can distort the signal while it passes. To be able to remove the distortion on the signal, we need to know what characteristics the channel carries, and in what way it distorts the signal.  Neural networks have become a popular scheme in channel estimation.
![The Process of Channel Estimation with Neural Networks](https://github.com/reyna07/chann_estimation/blob/main/img/ch_estimation_process.png)

In this project, the channel is imagined as a 2D matrix having two axes depicting the time and frequency response. This turns the channel estimation process into an image processing problem, where 2D CNNs are commonly used. To speed up the process of inference, VCK5000 is used. 

## Model and data acquisition
The Matlab example:[Deep Learning Data Synthesis for 5G Channel Estimation](https://www.mathworks.com/help/5g/ug/deep-learning-data-synthesis-for-5g-channel-estimation.html) provides a pre-trained channel evaluation model, input data for testing, and golden output data as a reference (perfect evaluation, actual channel realization).

## Project realization
The channel evaluation model generated by the MATLAB example is ported to the PyTorch environment to realize the inference process. In the testbench, the test signals generated by the MATLAB example are used as inputs of inference, and the outputs of the inference are compared with the golden outputs (perfect evaluation, realization of the actual channel provided by the MATLAB example). The testbench returns the MSE(Mean Square Error) of the inference output against the golden output.

Then, Vitis-AI Pytorch workflow is used for doing quantization and optimization of the model, and finally, implementing the model on VCK5000.

![The Project Steps Block Diagram](https://github.com/reyna07/chann_estimation/blob/main/img/chann_block.png)

## Setting up the project
1. Follow the [Getting started with VCK5000 Versal devices in OCT](https://github.com/OCT-FPGA/versal-tutorials/blob/main/vck5000-getting-started.md) to set the environment in Open Cloud Testbed. Important information: The workflow must be selected as #Vitis-AI# instead of #Vitis#.

   
2. Go to the Vitis-AI directory

```
cd /docker/Vitis-AI
```

3. Start the Docker container

```
./docker_run.sh xilinx/vitis-ai-pytorch-cpu:latest
```

4. Set up the Environment Variable after entering the Docker container.

```
source /workspace/board_setup/vck5000/setup.sh DPUCVDX8H_6pe_dwc
```

5. Clone this GitHub repository to the Vitis-AI directory

```
git clone https://github.com/reyna07/chann_estimation.git
```

## Software Simulation
1. Change into the `chann_estimation` directory
```
cd chann_estimation
```

2. Run the `run.sh` script to see the software simulation result:
```
chmod +x run.sh
./run.sh
```

Then, you can see the result, which shows the MSE between the SW simulation result and the golden output:

```
Prediction of the pretrained model has started...

Prediction finished!

Prediction file is ready!

Comparing against output data:

*****************************************************************************

PASS: The output matches the golden output!

Mean Squared Error between predicted and golden output: 0.0697585865855217

*****************************************************************************

```
## Hardware Simulation 

In this part, the development and deployment of a quantized neural network model for a DPU (Deep Processing Unit) is covered using the Vitis AI tools. The process involves several key steps, including model quantization, evaluation, compilation, and DPU inference. After setting the environment variables and downloading the necessary files by following the `Setting up the project` section, this section can be directly executed. 

1. Get into the `/workspace/chann_estimation/code` directory
```
cd /workspace/chann_estimation/code/
```

2. Run the quantization file in calibration mode to generate the configuration file. In calibration mode, `batch_size` is 32.
```
python quant_rev4.py --quant_mode 'calib' --batch_size 32
```

3. Run the quantization file in test with deploy mode to generate the xmodel. In test mode, the `batch_size` is 1 by default.
```
python quant_rev4.py --quant_mode 'test' --deploy
```
While generating and exporting the xmodel, the test mode also evaluates the model with dummy inputs and gives accuracy results. In the command window, results should be seen like this: 

``` 
Golden output dimension:torch.Size([8568])
Quantizer output dimension:torch.Size([8568])
Accuracy as Mean Squared Error (MSE): 0.13897
Loss: 0.36117637157440186


Model export completed!
Exporting test input data...

Test input data saved to test_input.txt
output dimension is:torch.Size([1, 1, 14, 612])
Test output data saved to test_output.txt

```

4. Compile the xmodel for DPU. 
```
vai_c_xir -x quantize_result/GraphModule_int.xmodel -a /opt/vitis_ai/compiler/arch/DPUCVDX8H/VCK50006PEDWC/arch.json -o chesti_pt -n chesti_pt
```
In above command, `quantize_result/GraphModule_int.xmodel` is the xmodel generated with quantization file. The architecture json file corresponding to the target setting is `/opt/vitis_ai/compiler/arch/DPUCVDX8H/VCK50006PEDWC/arch.json `. `chesti_pt` is the output directory in which the compiled xmodel will be placed with the name `chesti_pt.xmodel`. This compilation is done to  take the quantized INT8.xmodel and generate the deployable DPU.xmodel by running the command above.

5. Run the following command to generate the DPU inference result and compare the DPU inference result with the reference data automatically.
```
env XLNX_ENABLE_DUMP=1 XLNX_ENABLE_DEBUG_MODE=1 XLNX_GOLDEN_DIR=./quantize_result/deploy_check_data_int/GraphModule    xdputil run ./chesti_pt/chesti_pt.xmodel ./quantize_result/deploy_check_data_int/GraphModule/GraphModule__input_0.bin 2>result.log 1>&2
```
Here, `XLNX_GOLDEN_DIR=` sets the golden data, `./quantize_result/deploy_check_data_int/GraphModule` is the checking data directory(this directory contains many .bin files),
`xdputil run` runs the compiled xmodel in the path `./chesti_pt/chesti_pt.xmodel`.  `./quantize_result/deploy_check_data_int/GraphModule/GraphModule__input_0.bin` is the input bin data path. The results will be generated and saved in `result.log`.

6. Visualize the content of the result file `result.log`.
```
vim result.log
```
After executing this command, one should be able to see below lines: 
```
fillin GraphModule__input_0_fix
dump output to  0.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin
dump output to  1.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin
dump output to  2.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin
dump output to  3.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin
dump output to  4.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin
dump output to  5.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin

```
This indicates that the binary file `GraphModule__input_0_fix` was loaded as the input data for the model, and the final output from model operations was written to a binary file named `X.GraphModule__GraphModule_Conv2d_conv_5__ret_fix.bin`. Here, X index represents to sequence of the layer outputs. The rest of the binary file's name indicates that the fifth convolutional layer's output is being dumped. 
Quit the result.log file

## Runtime Compiling and Deployment

In this part, the created xmodel is deployed to the hardware, and the inference is done in the AI Engines of the VCK5000 board. After setting the environment variables and downloading the necessary files by following the `Setting up the project` section, this section can be directly executed. 

1. Get into the `/workspace/chann_estimation/code` directory.
   
```
cd /workspace/chann_estimation/code/
```

2.  Run the `build.sh script`, which calls the `ch_estimation_runner_2.cpp` and `ch_estimation_runner_3.cpp`. The .cpp files are C++ code that uses Vitis-AI C++ APIs, can compile the loading xmodel and input data into VCK5000, and save the output data process. The build.sh sets the compile environment, assign the compiled bit file name, and call the g++ compiler to compile these C++ codes.  `ch_estimation_runner_2.cpp` load the quantized xmodel and 2 original input data sets which are same as the software simulation data sets, then saved 2 output data sets to 2 text files. `ch_estimation_runner_3.cpp` load the test input data set which is generated in Quantization step.
```
chmod +x build.sh
./build.sh
```
Check the directory, you can see 2 runtime bit files: `ch_estimation_runner_2` and `ch_estimation_runner_3`.

3. Run `ch_estimation_runner_2` runtime bit file, then check the directory, you can see 2 output text files:`ouput1.txt` and `output2.txt`.
```
./ch_estimation_runner_2
```

4. Run `ch_estimation_runner_3` runtime bit file, then check the directory, you can see an output text files:`hw_test_ouput.txt`.
```
./ch_estimation_runner_3
```

5. Run the output data process Python code,
```
python hardware_result.py
```
Then you can see the MSE for 3 different hardware output:
```
MSE Loss of the hardware result for input1 is: 0.3863668739795685
MSE of the hardware result for input2 is: 0.393906831741333
MSE of the hardware result for test_input is: 0.38810738921165466
```

## Evaluation with VAI Profiler

The Vitis AI Profiler is a set of tools used to profile and visualize AI applications based on VART. To use VAI Profiler, first we need to use the vaitrace command to generate a .run_summary file and some .csv files. Then save these files to a local PC, and use Vitis Analyzer to monitor them. 
The command to use vaitrace:

```
vaitrace ./ch_estimation_runner_2 ./chesti_pt/chesti_pt.xmodel
```

Then, in a NERC machine, copy the directory of the files: 

```
scp -r -i ~/ssh/<your private key> <username>@<node id>.cloudlab.umass.edu:/docker/Vitis-AI/chann_estimation/code/  ~
```
An example command would be like this: 

```
scp -r -i ~/ssh/mykey bogrekci@pc178.cloudlab.umass.edu:/docker/Vitis-AI/chann_estimation/code/  ~
```
After copying the files, open Vitis Analyzer from  the command window. 

```
vitis_analyzer code/xrt.run_summary
```
Vitis Analyzer will open, and you will be able to visualize the Timeline Trace, Profile Summary, etc.

